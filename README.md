# Neural Networks and more

This is a collection of several practice scripts for implementation of object detection, neural network compression, distillation, pruning and large models.   
Details as follows:


## Rank Approximations and compression (JAX based)

Low Rank Approximations are used for few cases:

1.   To compress a Neural Network 
2.   To denoise a signal
3.   To impute missing values/find a structure to fill empty data

Singular Value Decomposition (SVD) exists for every rectangular matrix. Here in '', gentle implementation of SVD is scripted to compress a deep neural network.

Let ğ›¢ be a rectangular matrix of dimensions ğ‘šğ˜¹ğ‘›, then the SVD of the matrix A is given by $ A = Uğ›´V^T$ where $U$ is an orthogonal matrix of shape mxm containing the left singular vectors, $V$ is an orthogonal matrix of shape nxn containing the right singular vectors and $ğ›´$ is a diagonal matrix containing the singular values of $A$. This formulation of SVD can be re-expressed as \begin{align} A = \sum_{i=1}^{r} s_i. u_i v_i^T \end{align} where $r = \text{min}(m,n)$ represents the rank of the matrix, $s_i$ is the $i$th singular value and $u_i v_i^T$ is the outer product of the $i$th left and right singular vectors. 

\begin{align}
A = \sum_{i=1}^{\text{min}(m,n)} s_i. u_i v_i^T
\end{align}
\begin{align}

The singular values $ğ›´$ are decreasing in order. So, each outer product is scaled by a smaller value as we compute each term in the sum above. This gives us an opportunity to approximate $A$ using only the sum of the first $k$ outer products where $k < \text{min}(m,n)$ $-$ this effectively means that we are zero-ing out some of the singular values by assuming that the contribution to the sum is negligible. This is called low-rank approximation. 
