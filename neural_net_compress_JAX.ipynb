{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Low Rank Approximations"
      ],
      "metadata": {
        "id": "-oVsI7E5uVif"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoWy4-iQIYJ0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBFa6wXqAKgK"
      },
      "outputs": [],
      "source": [
        "!pip install dm-haiku optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzdlf_milUbR"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, Mapping, Tuple\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from absl import app\n",
        "import haiku as hk\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "import math\n",
        "from sklearn.decomposition import TruncatedSVD          # To calculate accuracy\n",
        "from statistics import mean\n",
        "\n",
        "Batch = Tuple[np.ndarray, np.ndarray]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqpgWQJs-evw"
      },
      "outputs": [],
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "def net_fn(batch: Batch) -> jnp.ndarray:\n",
        "\n",
        "  x = normalize(batch[0])\n",
        "  \n",
        "  # Architecture\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)), \n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)\n",
        "\n",
        "def load_dataset(split: str,*,is_training: bool,batch_size: int) -> Iterator[tuple]:     ###  ,*,\n",
        "\n",
        "  \"\"\"Loads the dataset as a generator of batches.\n",
        "    Args:\n",
        "    split : str \n",
        "      The split of the input dataset\n",
        "    is_training : bool\n",
        "      Dataset is to be trained or not\n",
        "    batch_size : int\n",
        "      Size of the batch\n",
        "\n",
        "    Returns: \n",
        "    Iterator : object\n",
        "      Returns Iterator of the given object\"\"\"\n",
        "\n",
        "  ds = tfds.load('cifar10', split=split, as_supervised=True).cache().repeat()           # tfds = TensorFlow Dataset\n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)       ### 10 * batch_size\n",
        "  ds = ds.batch(batch_size)\n",
        "  return iter(tfds.as_numpy(ds))\n",
        "\n",
        "def compute_loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "\n",
        "  \"\"\"Compute the loss of the network, including L2.\n",
        "\n",
        "    Args:\n",
        "\n",
        "     params : float32\n",
        "      The params of the network\n",
        "    batch : float32\n",
        "      Ground truth of network\n",
        "    \n",
        "    Returns: \n",
        "\n",
        "      softmax_xnet : float32\n",
        "        Returns loss\"\"\"\n",
        "\n",
        "  x,y = batch\n",
        "  logits = net.apply(params, batch)\n",
        "  preds = jax.nn.log_softmax(logits)     \n",
        "  labels = jax.nn.one_hot(y, 10)\n",
        "\n",
        "  l2_loss = jnp.sum(optax.l2_loss(preds,labels))           \n",
        "\n",
        "  #l2_loss = 16.0\n",
        "  weighted_l2_loss = 0.5 * l2_loss\n",
        "\n",
        "  softmax_xent = -jnp.sum(labels * preds)     # preds = jax.nn.log_softmax(logits)\n",
        "  softmax_xent = softmax_xent + (1e-4 * l2_loss)      ### Use of positive (+) instead of negative (-) for l2 regularization\n",
        "                                                      ### Learning rate can be 1e-3 instead of 1e-4 (accurate minimization for cost function)\n",
        "                                                      ### Use of weighted_l2_loss instead of l2_loss  (equal penalty for parameter)\n",
        "  return softmax_xent          \n",
        "\n",
        "@jax.jit\n",
        "def compute_accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "\n",
        "  \"\"\"Compute the accuracy of the network.\n",
        "\n",
        "  Args:\n",
        "\n",
        "    params : float32\n",
        "      The weights/params of network\n",
        "    batch : float32\n",
        "      Groud truth of network\n",
        "    \n",
        "  Returns: \n",
        "    accuracy : float32\n",
        "      Returns accuracy over the batch\"\"\"\n",
        "\n",
        "  x, y = batch\n",
        "  labels = jax.nn.one_hot(y,10)\n",
        "\n",
        "  predictions = net.apply(params, batch)\n",
        "  preds = jax.nn.log_softmax(predictions)\n",
        "\n",
        "  accuracy = jnp.mean(jnp.argmax(predictions,-1) == y) # compute accuracy over batch\n",
        "  #accuracy = 0.0\n",
        "  return accuracy\n",
        "\n",
        "@jax.jit\n",
        "def update(params: hk.Params, opt_state: optax.OptState, batch: Batch,) -> Tuple[hk.Params, optax.OptState]:\n",
        "\n",
        "  \"\"\"Update parameters of network.\n",
        "\n",
        "    Args:\n",
        "\n",
        "    params : float32\n",
        "      The parameters of network\n",
        "    opt_state : float32\n",
        "      Current state of network\n",
        "    batch : float32\n",
        "      Ground truth of input\n",
        "    \n",
        "  Returns: \n",
        "    new_params : float32\n",
        "      Updated parameters of network\n",
        "    opt_state : \n",
        "      Updates state of network\"\"\"\n",
        "  \n",
        "  grads = jax.grad(compute_loss)(params, batch)\n",
        "  updates, opt_state = opt.update(grads, opt_state)       # Update states\n",
        "  new_params = optax.apply_updates(params, -1*updates)\n",
        "\n",
        "  return new_params, opt_state\n",
        "\n",
        "@jax.jit\n",
        "def ema_update(params, avg_params):\n",
        "  \"\"\"Update parameter of network\n",
        "\n",
        "      Args:\n",
        "        params : float32\n",
        "          The parameters of network\n",
        "        avg_params :\n",
        "          Moving average of paramters of network\n",
        "\n",
        "      Returns:\n",
        "        params : float32\n",
        "          Updated parameters of network\n",
        "        avg_params :\n",
        "          Updated moving average of paramters of network\"\"\"\n",
        "\n",
        "  return optax.incremental_update(params, avg_params, step_size=0.001)\n",
        "\n",
        "def normalize(images):\n",
        "\n",
        "  \"\"\"Normalize the data\n",
        "\n",
        "  Args:\n",
        "    images : string\n",
        "      Path of input\n",
        "  \n",
        "  Returns:\n",
        "    x : float32\n",
        "    Normalizes ground truth of input\"\"\"\n",
        "  \n",
        "  mean = np.asarray(CIFAR10_MEAN)\n",
        "  std = np.asarray(CIFAR10_STD)\n",
        "\n",
        "  x = images.astype(jnp.float32) / 255.         ### float32 provides faster computation and approximation instead of int8 \n",
        "  x /- mean                  \n",
        "  x /= std\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5iI930cIjzM"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4-HuMSH_Cbw"
      },
      "outputs": [],
      "source": [
        "net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "\n",
        "# Learning rate\n",
        "opt = optax.adam(1e-3)\n",
        "# opt = optax.chain(optax.adam(1e-3), optax.scale_by_adam(), optax.scale(-1.0))       ### Use of optax in-built methods to\n",
        "                                                                                      ### and take learning rate reduction into account \n",
        "\n",
        "train = load_dataset(\"train[80%:]\", is_training=True, batch_size=1000)    \n",
        "validation = load_dataset(\"train[0%:80%]\", is_training=False, batch_size=10000)\n",
        "test = load_dataset(\"test\", is_training=False, batch_size=10000)\n",
        "\n",
        "params = avg_params = net.init(jax.random.PRNGKey(42), next(train))\n",
        "opt_state = opt.init(params)            \n",
        "\n",
        "# Do not alter the number of steps\n",
        "for step in range(10001):      #  For faster computation \n",
        "\n",
        "  if step % 1000 == 0:         # For faster computation\n",
        "    val_accuracy = compute_accuracy(avg_params, next(validation))\n",
        "    val_loss = compute_loss(avg_params, next(validation))\n",
        "    test_accuracy = compute_accuracy(avg_params, next(test))\n",
        "    val_accuracy, test_accuracy = jax.device_get(\n",
        "        (val_accuracy, test_accuracy))\n",
        "    print(f\"[Step {step}] Validation / Test accuracy: \"\n",
        "          f\"{val_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "    print(f\"[Step {step}] Loss: \" f\"{val_loss:.3f}.\")   \n",
        "\n",
        "  params, opt_state = update(params, opt_state, next(train))\n",
        "  avg_params = ema_update(params, avg_params)               # Switch states new_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and Functions"
      ],
      "metadata": {
        "id": "8Jomz9hTufls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdZqO7W_KSX8"
      },
      "outputs": [],
      "source": [
        "def compute_eval_metrics(params, batch, n_samples):\n",
        "\n",
        "  duration_list = []\n",
        "  accuracy_list = []\n",
        "  for i in range(n_samples):\n",
        "    start = time.time()\n",
        "    acc = compute_accuracy(params, batch)\n",
        "    end = time.time()\n",
        "    duration = end - start\n",
        "    duration_list.append(duration)\n",
        "    accuracy_list.append(acc)\n",
        "  # mean_acc = mean(accuracy_list)\n",
        "\n",
        "  return accuracy_list,duration_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8U8Nlp9IS4q"
      },
      "outputs": [],
      "source": [
        "def rank_approximated_weight(weight: jnp.ndarray, rank_fraction: float):\n",
        "\n",
        "  #weight = np.linalg.matrix_rank(weight)\n",
        "\n",
        "  U, S, V = np.linalg.svd(weight, full_matrices=False)        # SVD\n",
        "  k = rank = int(rank_fraction * min(len(weight[0]), len(weight[1])))\n",
        "  #print(k)\n",
        "  \n",
        "  #rank_matrix = np.zeros(len(U),len(V))\n",
        "  rank_matrix = (U[:,:k] @ np.diag(S[k-1])) @ V[:k]       # Find rank of a matrix\n",
        "  U, S, V = np.linalg.svd(rank_matrix, full_matrices=False)   # Reconstruct and find u,s,v\n",
        "\n",
        "  #S = np.diag(S[k-1])\n",
        "\n",
        "  # u = jax.random.normal(jax.random.PRNGKey(42), shape=weight.shape)\n",
        "  # size = weight.shape[1]\n",
        "  # v = jax.random.normal(jax.random.PRNGKey(42), shape=(size,size))\n",
        "\n",
        "  return U[:,k], V[:k], S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvm9InR4JG4F"
      },
      "source": [
        "### Evaluations \n",
        "At different ranks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEjLxaDPEGEY"
      },
      "outputs": [],
      "source": [
        "rank_truncated_params = deepcopy(params)\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1):\n",
        "\n",
        "  print(f\"Evaluating the model at {rank_fraction}\")\n",
        "  for layer in params.keys():\n",
        "    if 'conv' in layer:\n",
        "      continue\n",
        "    weight = params[layer]['w']\n",
        "    u, v, s = rank_approximated_weight(weight, rank_fraction)\n",
        "    rank_truncated_params[layer]['w'] = u@v\n",
        "\n",
        "  test_batch = next(test)\n",
        "  # we compute metrics over 50 samples to reduce noise in the measurement.\n",
        "  n_samples = 50\n",
        "  test_accuracy, latency = compute_eval_metrics(rank_truncated_params, next(test), n_samples)\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(latency):.4f}.\")\n",
        "  ranks_and_times.append((rank_fraction, np.mean(latency)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzyvqIr38eWw"
      },
      "source": [
        "### Plot relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCdYJ6lSEKM9"
      },
      "outputs": [],
      "source": [
        "# Accuracy vs Rank Percentage\n",
        "\n",
        "plt.plot(ranks_and_accuracies[0],ranks_and_accuracies[1])\n",
        "plt.label(\"Accuracy vs Rank Percentage\")\n",
        "plt.xlabel(\"Rank percentage (%)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7jlMYxhi7E-"
      },
      "outputs": [],
      "source": [
        "# Rank Time vs Rank Percentage\n",
        "\n",
        "plt.plot(ranks_and_times[0], ranks_and_times[1])\n",
        "plt.label(\"Rank Time vs Rank Percentage\")\n",
        "plt.xlabel(\"Rank Percentage (%)\")\n",
        "plt.ylabel(\"Rank Times\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluations\n",
        "At Factorized space"
      ],
      "metadata": {
        "id": "kyw50ALLv260"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctdUxerSZut"
      },
      "outputs": [],
      "source": [
        "def low_rank_net_fn(batch: Batch, rank: float) -> jnp.ndarray:\n",
        "  \n",
        "  x = normalize(batch[0])\n",
        "  total_input_dim = np.prod(x.shape[1:])\n",
        "\n",
        "  #  Architecture code.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(int(rank * min(total_input_dim, 3000)), with_bias=False),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,      \n",
        "      hk.Linear(int(rank * 1000), with_bias=False), \n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 10), with_bias=False),\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi2TN_WV-jgZ"
      },
      "outputs": [],
      "source": [
        "vanilla_to_low_rank_map = {\n",
        "    'conv2_d': 'conv2_d',\n",
        "    'conv2_d_1': 'conv2_d_1',\n",
        "    'linear': ['linear', 'linear_1'],\n",
        "    'linear_1': ['linear_2', 'linear_3'],\n",
        "    'linear_2': ['linear_4', 'linear_5'],\n",
        "    'linear_3': ['linear_6', 'linear_7'],\n",
        "    'linear_4': ['linear_8', 'linear_9']\n",
        "}\n",
        "\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1):\n",
        "  low_rank_net_fn_partial = partial(low_rank_net_fn, rank=rank_fraction)\n",
        "  net = hk.without_apply_rng(hk.transform(low_rank_net_fn_partial)) \n",
        "  low_rank_params = net.init(jax.random.PRNGKey(42), next(train))\n",
        "\n",
        "  print(f\"Evaluating the model at\" f\"{rank_fraction:.2f}\")\n",
        "\n",
        "  for layer in vanilla_to_low_rank_map.keys():\n",
        "    if 'conv' in layer:\n",
        "      low_rank_params[layer] = params[layer]\n",
        "      continue\n",
        "    weight = params[layer]['w']\n",
        "    u, v = rank_approximated_weight(weight, rank_fraction)\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][0]]['w'] = u\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['w'] = v\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['b'] = params[layer]['b']\n",
        "  \n",
        "  test_accuracy, duration = compute_eval_metrics(low_rank_params, next(test), 50)\n",
        "  ranks_and_times.append((rank_fraction, np.mean(duration)))\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(duration):.4f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcBKkogM6uV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-v-iHTH3nasL",
        "UHGqpn3tkFRL"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}